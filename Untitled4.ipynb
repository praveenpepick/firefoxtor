{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced Unified LLM Provider Interface\n",
    "Connects to 28+ LLM providers through OpenAI-compatible endpoints\n",
    "Supports manual API key configuration with updated models and providers\n",
    "Updated: September 2025\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Dict, List, Optional, Union, Any\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "class LLMProvider(Enum):\n",
    "    \"\"\"Enum for supported LLM providers - Updated with 28 providers\"\"\"\n",
    "    OPENAI = \"openai\"\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    GOOGLE = \"google\"\n",
    "    XAI = \"xai\"\n",
    "    CEREBRAS = \"cerebras\"\n",
    "    PERPLEXITY = \"perplexity\"\n",
    "    TOGETHER = \"together\"\n",
    "    OPENROUTER = \"openrouter\"\n",
    "    SAMBANOVA = \"sambanova\"\n",
    "    COHERE = \"cohere\"\n",
    "    CLOUDFLARE = \"cloudflare\"\n",
    "    HUGGINGFACE = \"huggingface\"\n",
    "    DEEPSEEK = \"deepseek\"\n",
    "    FIREWORKS = \"fireworks\"\n",
    "    HYPERBOLIC = \"hyperbolic\"\n",
    "    REPLICATE = \"replicate\"\n",
    "    MISTRAL = \"mistral\"\n",
    "    LEPTON = \"lepton\"\n",
    "    NOVITA = \"novita\"\n",
    "    GROQ = \"groq\"\n",
    "    ANYSCALE = \"anyscale\"\n",
    "    DEEPINFRA = \"deepinfra\"\n",
    "    AI21 = \"ai21\"\n",
    "    ALIBABA = \"alibaba\"\n",
    "    SILICONFLOW = \"siliconflow\"\n",
    "    ZAI = \"z.ai\"\n",
    "    NVIDIA = \"nvidia\"\n",
    "    BASETEN = \"baseten\"\n",
    "    AKASH =\"akash\"\n",
    "    MINIMAX=\"minimax\"\n",
    "\n",
    "@dataclass\n",
    "class ProviderConfig:\n",
    "    \"\"\"Configuration for each LLM provider\"\"\"\n",
    "    base_url: str\n",
    "    default_model: str\n",
    "    models: List[str]  # Available models\n",
    "    headers: Optional[Dict[str, str]] = None\n",
    "    api_key_prefix: Optional[str] = None  # Some providers need specific key prefixes\n",
    "\n",
    "# Enhanced provider configurations with updated models and new providers\n",
    "PROVIDER_CONFIGS = {\n",
    "    LLMProvider.OPENAI: ProviderConfig(\n",
    "        base_url=\"https://api.openai.com/v1\",\n",
    "        default_model=\"gpt-4o-mini\",\n",
    "        models=[\"gpt-5\", \"gpt-5-mini\", \"gpt-5-nano\", \"gpt-4o\", \"gpt-4o-mini\", \"gpt-4-turbo\", \"gpt-3.5-turbo\", \"o1-preview\", \"o1-mini\", \"o3\", \"o3-mini\"]\n",
    "    ),\n",
    "    LLMProvider.ANTHROPIC: ProviderConfig(\n",
    "        base_url=\"https://api.anthropic.com/v1\",\n",
    "        default_model=\"claude-sonnet-4-0\",\n",
    "        models=[\"claude-opus-4-1\", \"claude-opus-4-0\", \"claude-sonnet-4-0\", \"claude-3-5-sonnet-20241022\", \"claude-3-7-sonnet-latest\", \"claude-3-5-haiku-latest\", \"claude-3-opus-20240229\", \"claude-3-sonnet-20240229\", \"claude-3-haiku-20240307\"]\n",
    "    ),\n",
    "    LLMProvider.GOOGLE: ProviderConfig(\n",
    "        base_url=\"https://generativelanguage.googleapis.com/v1beta\",\n",
    "        default_model=\"gemini-2.5-flash\",\n",
    "        models=[\"gemini-2.5-pro\", \"gemini-2.5-flash\", \"gemini-2.5-flash-lite\", \"gemini-2.0-flash\", \"gemini-2.0-flash-lite\", \"gemini-1.5-pro\", \"gemini-1.5-flash\", \"gemini-1.0-pro\"]\n",
    "    ),\n",
    "    LLMProvider.XAI: ProviderConfig(\n",
    "        base_url=\"https://api.x.ai/v1\",\n",
    "        default_model=\"grok-beta\",\n",
    "        models=[\"grok-4\", \"grok-3\", \"grok-3-fast\", \"grok-3-mini\", \"grok-3-mini-fast\", \"grok-2-1212\", \"grok-2-vision-1212\", \"grok-beta\", \"grok-vision-beta\"]\n",
    "    ),\n",
    "    LLMProvider.CEREBRAS: ProviderConfig(\n",
    "        base_url=\"https://api.cerebras.ai/v1\",\n",
    "        default_model=\"llama3.3-70b\",\n",
    "        models=[\"llama3.3-70b\", \"llama3.1-70b\", \"llama3.1-8b\", \"qwen3-32b\", \"qwen3-8b\"]\n",
    "    ),\n",
    "    LLMProvider.PERPLEXITY: ProviderConfig(\n",
    "        base_url=\"https://api.perplexity.ai\",\n",
    "        default_model=\"llama-3.1-sonar-small-128k-online\",\n",
    "        models=[\"llama-3.1-sonar-small-128k-online\", \"llama-3.1-sonar-large-128k-online\", \"llama-3.1-sonar-small-128k-chat\", \"llama-3.1-sonar-large-128k-chat\"]\n",
    "    ),\n",
    "    LLMProvider.TOGETHER: ProviderConfig(\n",
    "        base_url=\"https://api.together.xyz/v1\",\n",
    "        default_model=\"openai/gpt-oss-120b\",\n",
    "        models=[\"meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo\", \"openai/gpt-oss-120b\", \"meta-llama/Meta-Llama-3.1-405B-Instruct-Turbo\", \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\", \"mistralai/Mixtral-8x22B-Instruct-v0.1\"]\n",
    "    ),\n",
    "    LLMProvider.OPENROUTER: ProviderConfig(\n",
    "        base_url=\"https://openrouter.ai/api/v1\",\n",
    "        default_model=\"google/gemini-2.5-flash-lite-preview-09-2025\",\n",
    "        models=[\"google/gemini-2.5-flash-lite-preview-09-2025\", \"anthropic/claude-3.5-sonnet\", \"openai/gpt-4o\", \"google/gemini-pro-1.5\", \"mistralai/mixtral-8x22b-instruct\", \"xai/grok-beta\"]\n",
    "    ),\n",
    "    LLMProvider.SAMBANOVA: ProviderConfig(\n",
    "        base_url=\"https://api.sambanova.ai/v1\",\n",
    "        default_model=\"Meta-Llama-3.1-70B-Instruct\",\n",
    "        models=[\"Meta-Llama-3.3-70B-Instruct\", \"Meta-Llama-3.1-70B-Instruct\", \"Meta-Llama-3.1-405B-Instruct\", \"Meta-Llama-3.1-8B-Instruct\"]\n",
    "    ),\n",
    "    LLMProvider.COHERE: ProviderConfig(\n",
    "        base_url=\"https://api.cohere.ai/v1\",\n",
    "        default_model=\"command-r-plus\",\n",
    "        models=[\"command-r-plus\", \"command-r\", \"command\", \"command-light\"]\n",
    "    ),\n",
    "    LLMProvider.CLOUDFLARE: ProviderConfig(\n",
    "        base_url=\"https://api.cloudflare.com/client/v4/accounts/{account_id}/ai/v1\",\n",
    "        default_model=\"@cf/meta/llama-3.3-70b-instruct\",\n",
    "        models=[\"@cf/meta/llama-3.3-70b-instruct\", \"@cf/meta/llama-3.1-8b-instruct\", \"@cf/mistral/mistral-7b-instruct-v0.2\", \"@cf/qwen/qwen1.5-14b-chat-awq\"]\n",
    "    ),\n",
    "    LLMProvider.HUGGINGFACE: ProviderConfig(\n",
    "        base_url=\"https://api-inference.huggingface.co/v1\",\n",
    "        default_model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "        models=[\"meta-llama/Meta-Llama-3-8B-Instruct\", \"mistralai/Mixtral-8x7B-Instruct-v0.1\", \"google/gemma-1.1-7b-it\", \"microsoft/Phi-3-mini-4k-instruct\"]\n",
    "    ),\n",
    "    LLMProvider.DEEPSEEK: ProviderConfig(\n",
    "        base_url=\"https://api.deepseek.com/v1\",\n",
    "        default_model=\"deepseek-chat\",\n",
    "        models=[\"deepseek-v3\", \"deepseek-chat\", \"deepseek-coder\", \"deepseek-reasoner\", \"janus-pro-7b\"]\n",
    "    ),\n",
    "    # New providers added\n",
    "    LLMProvider.FIREWORKS: ProviderConfig(\n",
    "        base_url=\"https://api.fireworks.ai/inference/v1\",\n",
    "        default_model=\"accounts/fireworks/models/llama-v3p1-70b-instruct\",\n",
    "        models=[\"accounts/fireworks/models/llama-v3p1-70b-instruct\", \"accounts/fireworks/models/llama-v3p1-405b-instruct\", \"accounts/fireworks/models/mixtral-8x22b-instruct\"]\n",
    "    ),\n",
    "    LLMProvider.HYPERBOLIC: ProviderConfig(\n",
    "        base_url=\"https://api.hyperbolic.xyz/v1\",\n",
    "        default_model=\"meta-llama/Meta-Llama-3.1-70B-Instruct\",\n",
    "        models=[\"meta-llama/Meta-Llama-3.1-70B-Instruct\", \"meta-llama/Meta-Llama-3.1-405B-Instruct\", \"Qwen/Qwen2.5-72B-Instruct\", \"deepseek-ai/DeepSeek-V2.5\"]\n",
    "    ),\n",
    "    LLMProvider.REPLICATE: ProviderConfig(\n",
    "        base_url=\"https://openai-proxy.replicate.com/v1\",\n",
    "        default_model=\"meta/llama-3-70b-instruct\",\n",
    "        models=[\"meta/llama-3-70b-instruct\", \"mistralai/mixtral-8x7b-instruct-v0.1\"]\n",
    "    ),\n",
    "    LLMProvider.MISTRAL: ProviderConfig(\n",
    "        base_url=\"https://api.mistral.ai/v1\",\n",
    "        default_model=\"mistral-small-latest\",\n",
    "        models=[\"pixtral-large-latest\", \"mistral-large-latest\", \"mistral-medium-latest\", \"mistral-medium-2505\", \"mistral-small-latest\", \"codestral-latest\", \"pixtral-12b-2409\"]\n",
    "    ),\n",
    "    LLMProvider.LEPTON: ProviderConfig(\n",
    "        base_url=\"https://api.lepton.ai/v1\",\n",
    "        default_model=\"llama3.1-70b\",\n",
    "        models=[\"llama3.1-70b\", \"llama3.1-8b\", \"mixtral-8x7b\", \"qwen2.5-72b\"]\n",
    "    ),\n",
    "    LLMProvider.NOVITA: ProviderConfig(\n",
    "        base_url=\"https://api.novita.ai/v3/openai\",\n",
    "        default_model=\"meta-llama/llama-3.1-70b-instruct\",\n",
    "        models=[\"meta-llama/llama-3.1-70b-instruct\", \"meta-llama/llama-3.1-8b-instruct\", \"mistralai/mistral-7b-instruct\", \"Qwen/Qwen2.5-72B-Instruct\"]\n",
    "    ),\n",
    "    LLMProvider.GROQ: ProviderConfig(\n",
    "        base_url=\"https://api.groq.com/openai/v1\",\n",
    "        default_model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "        models=[\"meta-llama/llama-4-scout-17b-16e-instruct\", \"llama-3.3-70b-versatile\", \"llama-3.1-70b-versatile\", \"llama3-70b-8192\", \"mixtral-8x7b-32768\", \"gemma2-9b-it\", \"llama-3.2-90b-text-preview\"]\n",
    "    ),\n",
    "    LLMProvider.ANYSCALE: ProviderConfig(\n",
    "        base_url=\"https://api.endpoints.anyscale.com/v1\",\n",
    "        default_model=\"meta-llama/Llama-3-70b-chat-hf\",\n",
    "        models=[\"meta-llama/Llama-3-70b-chat-hf\", \"meta-llama/Llama-3-8b-chat-hf\", \"mistralai/Mistral-7B-Instruct-v0.1\"]\n",
    "    ),\n",
    "    LLMProvider.DEEPINFRA: ProviderConfig(\n",
    "        base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    "        default_model=\"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "        models=[\"meta-llama/Meta-Llama-3-70B-Instruct\", \"meta-llama/Meta-Llama-3.1-405B-Instruct\", \"microsoft/WizardLM-2-8x22B\", \"Qwen/Qwen2.5-72B-Instruct\"]\n",
    "    ),\n",
    "    LLMProvider.AI21: ProviderConfig(\n",
    "        base_url=\"https://api.ai21.com/studio/v1\",\n",
    "        default_model=\"jamba-1.5-large\",\n",
    "        models=[\"jamba-1.5-large\", \"jamba-1.5-mini\", \"j2-ultra\", \"j2-mid\"]\n",
    "    ),\n",
    "    LLMProvider.ALIBABA: ProviderConfig(\n",
    "        base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "        default_model=\"qwen-plus\",\n",
    "        models=[\"qwen-plus\", \"qwen-turbo\", \"qwen-max\", \"qwen2.5-72b-instruct\", \"qwen2.5-32b-instruct\"]\n",
    "    ),\n",
    "    LLMProvider.SILICONFLOW: ProviderConfig(\n",
    "        base_url=\"https://api.siliconflow.cn/v1\",\n",
    "        default_model=\"Qwen/Qwen3-8B\",\n",
    "        models=[\"deepseek/DeepSeek-R1-Distill-Qwen-32B\", \"Qwen/Qwen2.5-72B-Instruct\", \"meta-llama/Meta-Llama-3.1-70B-Instruct\", \"google/gemma-2-9b-it\"]\n",
    "    ),\n",
    "    LLMProvider.ZAI: ProviderConfig(\n",
    "        base_url=\"https://api.z.ai/api/paas/v4/\",\n",
    "        default_model=\"glm-4.6\",\n",
    "        models=[\"glm-4.6\", \"gpt-4o\", \"claude-3-haiku\", \"gemini-1.5-flash\"]\n",
    "    ),\n",
    "    LLMProvider.NVIDIA: ProviderConfig(\n",
    "        base_url=\"https://integrate.api.nvidia.com/v1\",\n",
    "        default_model=\"meta/llama-3.1-70b-instruct\",\n",
    "        models=[\"meta/llama-3.1-70b-instruct\", \"meta/llama-3.1-405b-instruct\", \"nvidia/llama-3.1-nemotron-70b-instruct\", \"google/gemma-2-9b-it\", \"mistralai/mixtral-8x22b-instruct-v0.1\", \"microsoft/phi-3-medium-128k-instruct\"]\n",
    "    ),\n",
    "    LLMProvider.BASETEN: ProviderConfig(\n",
    "        base_url=\"https://api.baseten.co/v1\",\n",
    "        default_model=\"llama-3.1-70b-instruct\",\n",
    "        models=[\"llama-3.1-70b-instruct\", \"mixtral-8x7b-instruct\", \"gemma-2-9b-it\"]\n",
    "    ),\n",
    "    LLMProvider.AKASH: ProviderConfig(\n",
    "        base_url=\"https://chatapi.akash.network/api/v1\",\n",
    "        default_model=\"gpt-oss-120b\",\n",
    "        models=[\"gpt-oss-120b\", \"Meta-Llama-3-3-70B-Instruct\",]\n",
    "    ),\n",
    "    LLMProvider.MINIMAX: ProviderConfig(\n",
    "        base_url=\"https://api.minimax.io/v1\",\n",
    "        default_model=\"MiniMax-M2\",\n",
    "        models=[\"MiniMax-M2\", \"MiniMax-M3\",]\n",
    "    ),\n",
    "}\n",
    "\n",
    "class UnifiedLLMClient:\n",
    "    \"\"\"Enhanced unified client for 28+ LLM providers with manual API key configuration\"\"\"\n",
    "    \n",
    "    def __init__(self, provider: Optional[Union[str, LLMProvider]] = None, \n",
    "                 api_key: Optional[str] = None,\n",
    "                 base_url: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize the client for a specific provider\n",
    "        \n",
    "        Args:\n",
    "            provider: Provider name or enum\n",
    "            api_key: API key for the provider\n",
    "            base_url: Optional custom base URL\n",
    "        \"\"\"\n",
    "        self.active_provider = None\n",
    "        self.active_client = None\n",
    "        \n",
    "        if provider and api_key:\n",
    "            self.set_provider(provider, api_key, base_url)\n",
    "    \n",
    "    def set_provider(self, provider: Union[str, LLMProvider], \n",
    "                    api_key: str, \n",
    "                    base_url: Optional[str] = None) -> None:\n",
    "        \"\"\"\n",
    "        Set the active provider with API key\n",
    "        \n",
    "        Args:\n",
    "            provider: Provider name or enum\n",
    "            api_key: API key for the provider\n",
    "            base_url: Optional custom base URL (overrides default)\n",
    "        \"\"\"\n",
    "        # Convert string to enum if necessary\n",
    "        if isinstance(provider, str):\n",
    "            try:\n",
    "                provider = LLMProvider(provider.lower())\n",
    "            except ValueError:\n",
    "                raise ValueError(f\"Unknown provider: {provider}. Available: {[p.value for p in LLMProvider]}\")\n",
    "        \n",
    "        if provider not in PROVIDER_CONFIGS:\n",
    "            raise ValueError(f\"Provider {provider.value} not configured\")\n",
    "        \n",
    "        config = PROVIDER_CONFIGS[provider]\n",
    "        \n",
    "        # Use custom base_url if provided, otherwise use default\n",
    "        actual_base_url = base_url or config.base_url\n",
    "        \n",
    "        # Handle special cases\n",
    "        if provider == LLMProvider.CLOUDFLARE and \"{account_id}\" in actual_base_url:\n",
    "            # Extract account_id from api_key format: \"account_id:api_key\"\n",
    "            if \":\" in api_key:\n",
    "                account_id, actual_key = api_key.split(\":\", 1)\n",
    "                actual_base_url = actual_base_url.replace(\"{account_id}\", account_id)\n",
    "                api_key = actual_key\n",
    "            else:\n",
    "                raise ValueError(\"Cloudflare requires api_key in format 'account_id:api_key'\")\n",
    "        \n",
    "        # Add API key prefix if needed\n",
    "        if config.api_key_prefix and not api_key.startswith(config.api_key_prefix):\n",
    "            api_key = f\"{config.api_key_prefix}{api_key}\"\n",
    "        \n",
    "        try:\n",
    "            # Create OpenAI client with provider-specific configuration\n",
    "            self.active_client = OpenAI(\n",
    "                api_key=api_key,\n",
    "                base_url=actual_base_url\n",
    "            )\n",
    "            self.active_provider = provider\n",
    "            print(f\"âœ“ Set active provider: {provider.value}\")\n",
    "            print(f\"  Base URL: {actual_base_url}\")\n",
    "            print(f\"  Default model: {config.default_model}\")\n",
    "            print(f\"  Total models: {len(config.models)}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Failed to initialize {provider.value}: {e}\")\n",
    "    \n",
    "    def list_models(self) -> List[str]:\n",
    "        \"\"\"List available models for the active provider\"\"\"\n",
    "        if not self.active_provider:\n",
    "            raise ValueError(\"No active provider set. Call set_provider() first.\")\n",
    "        \n",
    "        return PROVIDER_CONFIGS[self.active_provider].models\n",
    "    \n",
    "    def get_providers_info(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Get information about all available providers\"\"\"\n",
    "        info = {}\n",
    "        for provider, config in PROVIDER_CONFIGS.items():\n",
    "            info[provider.value] = {\n",
    "                \"base_url\": config.base_url,\n",
    "                \"default_model\": config.default_model,\n",
    "                \"models\": config.models,\n",
    "                \"total_models\": len(config.models)\n",
    "            }\n",
    "        return info\n",
    "    \n",
    "    def list_all_providers(self) -> List[str]:\n",
    "        \"\"\"List all available provider names\"\"\"\n",
    "        return [provider.value for provider in LLMProvider]\n",
    "    \n",
    "    def search_models(self, query: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"Search for models across all providers by name\"\"\"\n",
    "        results = {}\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        for provider, config in PROVIDER_CONFIGS.items():\n",
    "            matching_models = [\n",
    "                model for model in config.models \n",
    "                if query_lower in model.lower()\n",
    "            ]\n",
    "            if matching_models:\n",
    "                results[provider.value] = matching_models\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def chat_completion(\n",
    "        self,\n",
    "        messages: List[Dict[str, str]],\n",
    "        model: Optional[str] = None,\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: Optional[int] = None,\n",
    "        stream: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a chat completion using the active provider\n",
    "        \n",
    "        Args:\n",
    "            messages: List of message dictionaries\n",
    "            model: Model name (uses default if not specified)\n",
    "            temperature: Sampling temperature\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            stream: Whether to stream the response\n",
    "            **kwargs: Additional provider-specific parameters\n",
    "        \n",
    "        Returns:\n",
    "            OpenAI ChatCompletion response or stream\n",
    "        \"\"\"\n",
    "        if not self.active_client:\n",
    "            raise ValueError(\"No active provider set. Call set_provider() first.\")\n",
    "        \n",
    "        # Use default model if not specified\n",
    "        if model is None:\n",
    "            model = PROVIDER_CONFIGS[self.active_provider].default_model\n",
    "        \n",
    "        # Validate model is available for this provider\n",
    "        available_models = PROVIDER_CONFIGS[self.active_provider].models\n",
    "        if model not in available_models:\n",
    "            print(f\"Warning: Model '{model}' may not be available. Available models: {available_models[:5]}...\")\n",
    "        \n",
    "        # Handle provider-specific adjustments\n",
    "\n",
    "        elif self.active_provider == LLMProvider.GOOGLE:\n",
    "            # Google Gemini might need specific handling\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            response = self.active_client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                stream=stream,\n",
    "                **kwargs\n",
    "            )\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {self.active_provider.value}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def quick_chat(self, prompt: str, **kwargs):\n",
    "        \"\"\"Quick helper method for single prompts\"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        return self.chat_completion(messages, **kwargs)\n",
    "\n",
    "    def get_provider_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"Get statistics about all providers\"\"\"\n",
    "        total_models = sum(len(config.models) for config in PROVIDER_CONFIGS.values())\n",
    "        provider_count = len(PROVIDER_CONFIGS)\n",
    "        \n",
    "        return {\n",
    "            \"total_providers\": provider_count,\n",
    "            \"total_models\": total_models,\n",
    "            \"average_models_per_provider\": round(total_models / provider_count, 1)\n",
    "        }\n",
    "\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize client\n",
    "    client = UnifiedLLMClient()\n",
    "    \n",
    "    client.set_provider(\"openai\", \"test\")\n",
    "    response = client.quick_chat(\"Hello, how are you?\")\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef794372-54d5-4b82-846e-779d297ab0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    " \n",
    "client = OpenAI(\n",
    "    api_key = \"test\",\n",
    "    base_url = \"https://llm.chutes.ai/v1/\",\n",
    ")\n",
    " \n",
    "completion = client.chat.completions.create(\n",
    "    model = \"deepseek-ai/DeepSeek-R1\",\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Kimi, an AI assistant provided by Moonshot AI. You are proficient in English conversations. You provide users with safe, helpful, and accurate answers. You will reject any questions involving terrorism, racism, or explicit content. Moonshot AI is a proper noun and should not be translated.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, my name is Li Lei. What is 1+1?\"}\n",
    "    ],\n",
    "    temperature = 0.6,\n",
    ")\n",
    " \n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b8d8aa-4d0c-43e6-9e14-38f98ed35a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3b490f-8497-4e76-b917-8864fff72cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# Replace with your Azure OpenAI resource details\n",
    "client = AzureOpenAI(\n",
    "            azure_endpoint=\"test.azure.com/\",\n",
    "            api_version=\"2025-01-01-preview\",\n",
    "            api_key=\"\"\n",
    "        )\n",
    "\n",
    "# Example: Chat Completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Tora-AI-Scholar\",  # e.g., \"gpt-4o\"\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is your model no and today date\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98e8ce1-4d69-4608-9e83-87603ec8ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://openrouter.ai/api/v1/credits\"\n",
    "\n",
    "headers = {\"Authorization\": \"Bearer test\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b886e-a723-4bb8-97a1-ea2ba0e19001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e461976-d1b3-4e25-93a3-130f4bee753b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
